<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Introducing easy HW acceleration for PyTorch | Sehyun Choi</title> <meta name="author" content="Sehyun Choi"> <meta name="description" content="Multi-GPU / TPU setting with PyTorch made easy with huggingface accelerate."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/syncdoth/jekyll-pygments-themes@master/onedark.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%91%F0%9F%8F%BB%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="syncdoth.github.io/blog/2021/hf_accelerate/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Sehyun </span>Choi</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/courseworks/">courseworks</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Introducing easy HW acceleration for PyTorch</h1> <p class="post-meta">July 30, 2021</p> <p class="post-tags"> <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fas fa-hashtag fa-sm"></i> machine-learning</a>     ·   <a href="/blog/category/tutorial"> <i class="fas fa-tag fa-sm"></i> Tutorial</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="multi-gpu-with-pytorch">Multi-GPU with PyTorch</h2> <p>The two most popular deep learning frameworks are arguably Tensorflow and PyTorch. There are many difference and similarities between the two, and one notable difference is about HW acceleration device management. For Tensorflow, after they updated their API to V2.0, most of the device management is done automatically: in most cases, the default behaviour is to use all the available gpus possible, and no explicit device placement code for each tensor object is needed. However, in PyTorch, the default is to not use any HW acceleration, and the device placement for each tensor is manual. This makes the tensor’s placement explicit and easy to follow, yet difficult and tedious in many occasions. For example, multi-GPU setting or TPU setting with PyTorch requires quite a bit of knowledge about multi-process (or multi-threaded) programming, distributed computing, etc. all of which are done in the background of the tensorflow engine.</p> <h2 id="introducing-accelerate">Introducing accelerate</h2> <p>Recently, Huggingface released a library called <a href="https://github.com/huggingface/accelerate" rel="external nofollow noopener" target="_blank">accelerate</a>, which, as the name suggests, contains ready-to-use APIs for hardware accelerations. What’s notable about this library is that it only requires few lines of code changed from vanilla PyTorch code! An example from the official <a href="https://github.com/huggingface/accelerate/blob/main/README.md" rel="external nofollow noopener" target="_blank">repo</a>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td> <td class="code"><pre>  <span class="kn">import</span> <span class="n">torch</span>
  <span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
  <span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="o">+</span> <span class="kn">from</span> <span class="n">accelerate</span> <span class="kn">import</span> <span class="n">Accelerator</span>

<span class="o">-</span> <span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="o">+</span> <span class="n">accelerator</span> <span class="o">=</span> <span class="nc">Accelerator</span><span class="p">()</span>

<span class="o">-</span> <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Transformer</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="o">+</span> <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Transformer</span><span class="p">()</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>

  <span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">'</span><span class="s">my_dataset</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="o">+</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">accelerator</span><span class="p">.</span><span class="nf">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

  <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">source</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
<span class="o">-</span>         <span class="n">source</span> <span class="o">=</span> <span class="n">source</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="o">-</span>         <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

          <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

          <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

<span class="o">-</span>         <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="o">+</span>         <span class="n">accelerator</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

          <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</pre></td> </tr></tbody></table></code></pre></figure> <h3 id="structure">Structure</h3> <hr> <p><em>Currently, the documentation of <code class="language-plaintext highlighter-rouge">accelerate</code> is not complete: except for the main</em> <em>class and examples, there is almost no documentation for different classes.</em></p> <hr> <p>The main class is <code class="language-plaintext highlighter-rouge">Accelerator</code>, defined <a href="https://github.com/huggingface/accelerate/blob/main/src/accelerate/accelerator.py" rel="external nofollow noopener" target="_blank">here</a>. The main methods include:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">prepare</code>: it prepares the model, dataloader, and optimizer for the specified device configuration. After this, the model is wrapped as <code class="language-plaintext highlighter-rouge">torch.DistributedDataParallel</code> object.</li> <li> <code class="language-plaintext highlighter-rouge">backward</code>: instead of <code class="language-plaintext highlighter-rouge">loss.backward()</code>, <code class="language-plaintext highlighter-rouge">accelerator.backward(loss)</code> should be called.</li> <li> <code class="language-plaintext highlighter-rouge">print</code>: instead of plain python <code class="language-plaintext highlighter-rouge">print()</code>, <code class="language-plaintext highlighter-rouge">accelerator.print()</code> should be called, so that it is printed only once in the main process.</li> <li> <code class="language-plaintext highlighter-rouge">gather</code>: gathers tensors in different devices into one place.</li> <li> <code class="language-plaintext highlighter-rouge">save</code>: instead of <code class="language-plaintext highlighter-rouge">torch.save(model, path)</code>, <code class="language-plaintext highlighter-rouge">accelerator.save(model, path)</code> should be called. This saves the model only once in the main process.</li> </ul> <p><code class="language-plaintext highlighter-rouge">gather</code> method is mostly used with <strong>distributed evaluation</strong>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td> <td class="code"><pre>  <span class="n">preds</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">targets</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">validation_dataloader</span><span class="p">:</span>
      <span class="n">predictions</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
      <span class="c1"># Gather all predictions and targets
</span>      <span class="n">preds</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">accelerator</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">predictions</span><span class="p">))</span>
      <span class="n">targets</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">accelerator</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">targets</span><span class="p">))</span>

  <span class="c1"># perform evaluation...</span>
</pre></td> </tr></tbody></table></code></pre></figure> <h3 id="tips">Tips</h3> <p>Always remember that the <code class="language-plaintext highlighter-rouge">model</code> that you <code class="language-plaintext highlighter-rouge">prepare</code>d is now a <code class="language-plaintext highlighter-rouge">torch.DistributedDataParallel</code> object; you will not be able to call class specific methods of the <code class="language-plaintext highlighter-rouge">model</code>, such as <code class="language-plaintext highlighter-rouge">save_pretrained</code> of the huggingface <code class="language-plaintext highlighter-rouge">transformers.PretrainedModel</code> class. Also, note that the model saved with <code class="language-plaintext highlighter-rouge">accelerator.save</code> is also a <code class="language-plaintext highlighter-rouge">torch.DistributedDataParallel</code> object, and requires distributed setting when loading, which might be a little bit of a pain in the neck. You can refer to this <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-checkpoints" rel="external nofollow noopener" target="_blank">tutorial</a> for what setup is needed to load the model.</p> <p>It is difficult to avoid this problem without looking at the source code: referring to the <code class="language-plaintext highlighter-rouge">accelerator.save</code> at <a href="https://github.com/huggingface/accelerate/blob/b08fd560a4d6b7427f9fbb51a767393699afbd95/src/accelerate/utils.py#L270" rel="external nofollow noopener" target="_blank">here</a>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td> <td class="code"><pre>  <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
      <span class="sh">"""</span><span class="s">
      Save the data to disk. Use in place of :obj:`torch.save()`.
      Args:
          obj: The data to save
          f: The file (or file-like object) to use to save the data
      </span><span class="sh">"""</span>
      <span class="k">if</span> <span class="nc">AcceleratorState</span><span class="p">().</span><span class="n">distributed_type</span> <span class="o">==</span> <span class="n">DistributedType</span><span class="p">.</span><span class="n">TPU</span><span class="p">:</span>
          <span class="n">xm</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
      <span class="k">elif</span> <span class="nc">AcceleratorState</span><span class="p">().</span><span class="n">local_process_index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
          <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Note that <code class="language-plaintext highlighter-rouge">AcceleratorState()</code> is not a class bound property, but a singleton class that all instances of this class have the same state. Therefore, we can write a custom code to save the actual model wrapped by <code class="language-plaintext highlighter-rouge">torch.DistributedDataParallel</code>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td> <td class="code"><pre>  <span class="kn">from</span> <span class="n">accelerate.state</span> <span class="kn">import</span> <span class="n">AcceleratorState</span>
  <span class="kn">from</span> <span class="n">accelerate.utils</span> <span class="kn">import</span> <span class="n">save</span>

  <span class="k">def</span> <span class="nf">custom_save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">save_module_only</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
      <span class="sh">"""</span><span class="s">
      Save the data to disk. Use in place of :obj:`torch.save()`.
      Args:
          model: pytorch model. DistributedDataParallel class.
          f: The file (or file-like object) to use to save the data
          save_module_only: save the wrapped module only.
      </span><span class="sh">"""</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">save_module_only</span><span class="p">:</span>
          <span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
          <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">module</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

  <span class="c1"># save
</span>  <span class="nf">custom_save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="sh">'</span><span class="s">model.pt</span><span class="sh">'</span><span class="p">,</span> <span class="n">save_module_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="c1"># ...
</span>  <span class="c1"># load
</span>  <span class="n">loaded_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">model.pt</span><span class="sh">'</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></figure> <h2 id="alternatives">Alternatives</h2> <ul> <li> <p>Huggingface <code class="language-plaintext highlighter-rouge">transformers</code> already provides <code class="language-plaintext highlighter-rouge">Trainer</code> module, which takes care of hardware accelerations automatically. However, this is pretty much made only for the models provided in the <code class="language-plaintext highlighter-rouge">transformers</code> library, so it is less general.</p> </li> <li> <p>Another great alternative is <a href="https://www.pytorchlightning.ai/" rel="external nofollow noopener" target="_blank">pytorch-lightning</a> module. This is analogous to <code class="language-plaintext highlighter-rouge">keras</code> library made for PyTorch: the actual gradient update procedures are hidden from the developers, allowing them to focus only on the model’s architecture. This is indeed a great module and provides much more than just hardware acceleration; logging, tensorboard, checkpointing, training loop, and many other non-modeling codes are provided with simple options to the API. The only downside is that one has to refactor the code quite significantly to the required format of the module, which might not be feasible in the later stages of the project. For example, when I am working on a NLP related task, my go-to model would be <code class="language-plaintext highlighter-rouge">bert-base-cased</code>. This model can fit quite well within one GPU with 12GB of vRAM, so I wouldn’t worry about multi-gpu when I start coding up the project. However, as the project proceeds, I might require larger models, longer text, larger batch size to save time, and whatever reason that calls for multi-gpu setting. In that case, completely revamping the code to fit with <code class="language-plaintext highlighter-rouge">pytorch-lightning</code> module might be too much of a work, while using <code class="language-plaintext highlighter-rouge">accelerate</code> requries only a few lines of code modified.</p> </li> </ul> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme="light",giscusAttributes={src:"https://giscus.app/client.js","data-repo":"syncdoth/syncdoth.github.io","data-repo-id":"MDEwOlJlcG9zaXRvcnkzNDYwMzEyNDQ=","data-category":"General","data-category-id":"DIC_kwDOFKAEjM4CUVdE","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2023 Sehyun Choi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: July 14, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
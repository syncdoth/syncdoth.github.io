<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Big NLP Trends from 2016 to 2022 | Sehyun Choi</title> <meta name="author" content="Sehyun Choi"> <meta name="description" content="Recent trends in NLP from Attention to ChatGPT"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/syncdoth/jekyll-pygments-themes@master/onedark.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%91%F0%9F%8F%BB%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="syncdoth.github.io/blog/2023/big-nlp-trends/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Sehyun </span>Choi</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/courseworks/">courseworks</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Big NLP Trends from 2016 to 2022</h1> <p class="post-meta">February 20, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/nlp"> <i class="fas fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/tag/lm"> <i class="fas fa-hashtag fa-sm"></i> LM</a>   <a href="/blog/tag/ai"> <i class="fas fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/tag/ml"> <i class="fas fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/deep-learning"> <i class="fas fa-hashtag fa-sm"></i> Deep-learning</a>     ·   <a href="/blog/category/review"> <i class="fas fa-tag fa-sm"></i> Review</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="disclaimer">Disclaimer</h2> <p>This post may include some personal interpretation of the technologies introduced in here. Some of the information might be wrong. Feel free to leave a comment!</p> <h3 id="pre-2016-seq2seq">Pre 2016: Seq2Seq</h3> <p>In the Deep Natural Language Processing field, RNN was dominating the research community. The most notable model architecture is Sequence to Sequence (Seq2Seq), which used a bidirectional RNN encoder and an auto-regressive RNN decoder.</p> <h3 id="2016-attention">2016: Attention</h3> <p>An intuitive neural module called Attention was developed in 2016, and showed great improvement on Machine Translation task when applied on top of the encoder hidden states of Seq2Seq models.</p> <h3 id="2017-transformers">2017: Transformers</h3> <p>In a truly ground-breaking paper “Attention is All You Need”, a model architecture called Transformers was introduced, which substituted all recurrent operation over the sequences of RNN with a technique called self-attention.</p> <h3 id="2018-gpt-bert---era-of-pre-trained-language-models">2018: GPT, BERT - Era of Pre-Trained Language Models</h3> <p>2018 was the era of Pre-Trained Language Models (PTLMs), which started with Generative PreTraining (GPT) models, by OpenAI. This model only had transformer decoder stacks, and was trained using traditional, unidirectional Language Modeling Objective, or Next Token Prediction.</p> <p>Google soon followed with Bidirectional Encoder Representations from Transformers (BERT), with a novel task of Masked Language Modeling (MLM). Given a text, 30% of the tokens (words) were randomly masked, and the model was trained to predict what the tokens are given other words in the text. BERT is a bidirectional encoder-only architecture.</p> <p>GPT is trained to predict next token based on previous tokens, so it can be used to generate text, auto-regressively. Therefore, it became the signature model for Natural Language Generation (NLG).</p> <p>On the other Hand, BERT’s MLM objective allowed the model to model both the forward (next token given previous tokens) and backward (previous token given future tokens) contexts, (hence, bidirectional), it could obtain higher performance in text-classification-style tasks, dominating most of Natural Language Understanding (NLU) benchmarks.</p> <p>Note that both next token prediction and MLM objective do not require any label, but vast amount of text corpus. Therefore, this method was called self-supervised learning (SSL). Both BERT and GPT were pre-trained using SSL on large corpus and obtained considerable amount of natural language knowledge encoded within them. From this point on, the major direction of research was to</p> <ol> <li>Create another PTLM <ul> <li>This entails one or many of the following: <ul> <li>creating a new dataset (cleaner &amp; larger)</li> <li>creating a new objective</li> <li>creating a new model architecture</li> </ul> </li> </ul> </li> <li>Fine-tune the PTLM for another task <ul> <li>Fine-tuning refers to the training of Pre-Trained models on a smaller, task-oriented dataset. This approach achived much better result than training a new model from scratch on the task dataset.</li> </ul> </li> </ol> <h3 id="2019-larger-models">2019: Larger Models</h3> <p>Better PTLMs, with more dataset and more parameters were released, such as GPT2, roberta-large, etc. They went to upwards of <strong>2~3B</strong> parameters.</p> <ul> <li>In today’s terms, 2~3B is not “large” at all.</li> </ul> <h2 id="2020-gpt3-t5">2020: GPT3, T5</h2> <h3 id="gpt3">GPT3</h3> <p>OpenAI’s GPT3 was released. However, GPT3 was not just a model; it brought forth a lot of changes in the research directions of NLP.</p> <h4 id="closed-source">Closed Source</h4> <p>GPT3 was released as a closed-source, API only model, that users have to pay. This was the moment where OpenAI stopped being open.</p> <h4 id="pretraining-with-code-data">Pretraining with Code data</h4> <p>GPT3 pretraining data include code from github. Training on code data allowed the model to obtain some level of abstractive reasoning abilities.</p> <ul> <li>This led to the launch of Co-Pilot too.</li> </ul> <h4 id="rise-of-llm--in-context-learning">Rise of LLM &amp; In-Context Learning</h4> <p>The term Large Language Models (LLMs) started to catch fire, since GPT3 was truly large, with whopping <strong>175B</strong> parameters. With such huge scale came an unexpected ability of these models, which was called few-shot exemplar learning, which later was generalized to a term called In-Context Learning.</p> <p>famous LLMs are:</p> <ul> <li>OPT (facebook, 175B)</li> <li>PALM (GOogle, 540B)</li> <li>GPT-NeoX / GPT-J (EleutherAI)</li> <li>Bloom (Bigscience)</li> </ul> <h4 id="prompt-engineering">Prompt Engineering</h4> <p>Since GPT3 was only accessible through an API that takes textual input, what text prompt that goes into the model was important to obtain better output. This led to a subfield named prompt engineering, where various algorithms were proposed to find a textual prompt that would allow users to use GPT3 (or sometimes other models) better.</p> <ul> <li>This later inspired the rise of Prompt Tuning</li> </ul> <h3 id="t5">T5</h3> <p>All tasks represented in text and solved with single transformer encoder-decoder architecture.</p> <ul> <li>I think this laid the foundation for instruction tuning later.</li> <li>There is a similar model called T0 from Bigscience, which is pretrained to allow zero-shot inference on many NLP tasks using the same architecture as T5.</li> </ul> <h3 id="2021">2021</h3> <h4 id="parameter-efficient-fine-tuning-peft">Parameter Efficient Fine-Tuning (PEFT)</h4> <p>With the SOTA models being extra large, it was increasingly difficult to train these models on reasonable amount of GPUs. Also, tuning the entire model often resulted in problems like catastrophic forgetting. Researchers started to come up with ideas later organized as <strong>Parameter Efficient Fine-Tuning (PEFT)</strong>, which freezes the entire model weights except for a few parts (or introduce some small extra weights) and only tune that part on the task-oriented data.</p> <p>Most of the time, PEFT under-performed full finetuning. However, Prompt Tuning showed above SOTA performance in many benchmarks, and took the community by a storm.</p> <p>Prompt Tuning, or Soft Prompt Tuning, is a method of tuning a set of continuous vectors in the embedding / latent space of the model, that are prepended to the text embedding matrix. This way, the model itself is entirely frozen, and only a small set of vectors prepended to the input are trained using the gradient that flows through the model. This method achieved SOTA in many NLU tasks, taking the NLP trend of 2021.</p> <h4 id="chain-of-thought-prompting">Chain of Thought Prompting</h4> <p>This prompting guided the model to “think step by step”: this allowed the model to reason sequential steps to solving a problem, resulting in a more coherent reasoning of hard problems.</p> <h3 id="2022-instruction-tuning">2022: Instruction Tuning</h3> <p>Instruction tuning is sometimes called an alignment task: aligning language models with human usage of them. Most of the time, users “asks” or “instructs” the language models to generate some text; therefore, training these models to follow textual instruction can greatly increase their usability.</p> <p>Similar to the idea of T5, all different NLP tasks are represented as text, prepended with a natural language instruction of how to solve the task. This way, the LMs can model the dependency between the instruction, problem, and the answer, and since all of them are text, they become stronger at generalization and obtain strong zero-shot abilities.</p> <p>Notable models are:</p> <ul> <li>OPT-IML (Facebook)</li> <li>FLAN, Flan-T5 (Google)</li> <li>InstructGPT (GPT 3.5 family), ChatGPT (OpenAI)</li> <li>T0 (Bigscience, kind of)</li> </ul> <h3 id="chatgpt">ChatGPT</h3> <p>ChatGPT deserves its own section, because it was THE AI Phenomenon of 2000s.</p> <p>On top of InstructGPT, it was trained using Reinforcement Learning from Human Feedbacks (RLHF); basically, they hired a lot of human annotators (allegedly 1000) to “talk” with the current state of the model and give feedback in the form of score &amp; suggested answer. This way, ChatGPT could really generate truly human-level text. Also, thanks to 175B parameter scale, it stored tremendous amount of world knowledge, and can nearly be used as a search engine.</p> <p>However, it is still limited in that it “hallucinates”: it generates very plausible text that is entirely false.</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme="light",giscusAttributes={src:"https://giscus.app/client.js","data-repo":"syncdoth/syncdoth.github.io","data-repo-id":"MDEwOlJlcG9zaXRvcnkzNDYwMzEyNDQ=","data-category":"General","data-category-id":"DIC_kwDOFKAEjM4CUVdE","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2023 Sehyun Choi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: July 13, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="syncdoth.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="syncdoth.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-07-12T19:37:11+00:00</updated><id>syncdoth.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal page for showcasing my academic history. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Compositional Generalization</title><link href="syncdoth.github.io/blog/2023/compositional-generalization/" rel="alternate" type="text/html" title="Compositional Generalization"/><published>2023-07-11T11:40:00+00:00</published><updated>2023-07-11T11:40:00+00:00</updated><id>syncdoth.github.io/blog/2023/compositional%20generalization</id><content type="html" xml:base="syncdoth.github.io/blog/2023/compositional-generalization/"><![CDATA[<p>Compositional generalization has a quite long history. However, I would like to briefly review recent works that caught my eye, and how important they are.</p> <h2 id="what-is-it">What is it?</h2> <p>Compositional Generalization is about <em>can AI models generalize using different</em> <em>sub-knowledge in a compositional way to solve a more complex task?</em> A simple example from the well-known benchmark <a href="https://arxiv.org/abs/1711.00350">SCAN</a> is: ‚ÄúIf you know <code class="language-plaintext highlighter-rouge">JUMP</code>, <code class="language-plaintext highlighter-rouge">WALK</code>, and <code class="language-plaintext highlighter-rouge">WALK TWICE</code>, you should be able to understand <code class="language-plaintext highlighter-rouge">JUMP TWICE</code>.‚Äù Conventional Seq2seq methods (as of 2020, uni-RNN, bi-RNN, Transformers) all failed dramatically in this task.</p> <h2 id="why-is-it-important">Why is it important?</h2> <p>Compositional generalization can be interpreted or is related to a lot of different fields, such as OOD robustness (generalization direction), disentaglement (compositional direction), logical reasoning, or distantly, transfer learning. Since each of them has shown remarkable successes in different fields, I think this task has not been as hot as it could be. Moreover, one of the ‚Äúemergent abilities‚Äù of LLMs include showing traces of some level of compositional generalization ability, such as being able to translate from language A -&gt; B when it has been only trained on A -&gt; C and C -&gt; B / B -&gt; C pairs (forgot the source, will update üòÖ).</p> <p>However, it is still an important task because it is what makes the AI reach human level reasoning. As Noam Chomsky puts it, human language has ‚Äúinfinite uses of finite means.‚Äù Humans compose finite set of meanings, tools, functions, etc. into infinite possibilities, which makes us creative.</p> <h2 id="does-current-llm-has-it">Does Current LLM has it?</h2> <p>This is extensively explored by a recent UW paper: <a href="https://arxiv.org/abs/2305.18654">Faith and Fate: Limits of Transformer Compositionality</a>. The short answer is: No.</p> <h2 id="can-neural-networks-do-it">Can Neural Networks do it?</h2> <p>Yes and No. Some works such as LANE solve the SCAN generalization task to 100% accuracy. However, <a href="https://arxiv.org/abs/2307.03381">Teaching Arithmentics to Small Transformers</a> shows that it fails to generalize for arithmetic in longer digits. <a href="https://arxiv.org/abs/2305.18654">Faith and Fate: Limits of Transformer Compositionality</a> also shows that as the reasoning chain gets wider and deeper, Transformers (or similar auto-regressive sequence models) is likely to fail.</p> <h3 id="rwkv-4-demo">RWKV-4 Demo</h3> <p>A recent <a href="https://twitter.com/BlinkDL_AI/status/1677593798531223552?s=20">tweet</a> from BlinkDL (developer of RWKV, a very cool project that builds 100% RNN-based LM!) showed that a small RWKV model can solve high-digit arithmetics, trained similarly (with reversed digit tricks) as in <a href="https://arxiv.org/abs/2307.03381">Teaching Arithmentics to Small Transformers</a>.</p> <p>I ran a quick experiment on it: (code <a href="https://t.co/7HXdo9Tulw">link</a>) can this generalize to longer digits? The findings suggest <strong>NO</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RWKV_math_10x10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RWKV_math_10x10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RWKV_math_10x10-1400.webp"/> <img src="/assets/img/RWKV_math_10x10.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> RWKV-4Neo 2.9M model, trained from scratch by BlinkDL (maintainer), evaluated by me. Used 100 random examples per each k1 digits x k2 digits. </div> <h2 id="what-do-we-need-compositional-generalization">What do we need compositional generalization?</h2> <p>We don‚Äôt know it for sure yet. But, we know that simple MLE + Seq2Seq is not going to cut it. Either we need some auxiliary tasks, different loss, or different modules.</p> <h3 id="must-read">Must Read:</h3> <ul> <li><a href="https://arxiv.org/abs/2305.18654">Faith and Fate: Limits of Transformer Compositionality</a></li> <li><a href="https://arxiv.org/abs/2307.03381">Teaching Arithmentics to Small Transformers</a></li> <li><a href="https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v4neo/math_demo">RWKV Math Demo</a></li> </ul>]]></content><author><name></name></author><category term="Review"/><category term="NLP"/><category term="LM"/><category term="AI"/><category term="ML"/><category term="Deep-learning"/><summary type="html"><![CDATA[What is compositional generalization problem and why is this interesting?]]></summary></entry><entry><title type="html">Big NLP Trends from 2016 to 2022</title><link href="syncdoth.github.io/blog/2023/big-nlp-trends/" rel="alternate" type="text/html" title="Big NLP Trends from 2016 to 2022"/><published>2023-02-20T11:40:00+00:00</published><updated>2023-02-20T11:40:00+00:00</updated><id>syncdoth.github.io/blog/2023/big-nlp-trends</id><content type="html" xml:base="syncdoth.github.io/blog/2023/big-nlp-trends/"><![CDATA[<h2 id="disclaimer">Disclaimer</h2> <p>This post may include some personal interpretation of the technologies introduced in here. Some of the information might be wrong. Feel free to leave a comment!</p> <h3 id="pre-2016-seq2seq">Pre 2016: Seq2Seq</h3> <p>In the Deep Natural Language Processing field, RNN was dominating the research community. The most notable model architecture is Sequence to Sequence (Seq2Seq), which used a bidirectional RNN encoder and an auto-regressive RNN decoder.</p> <h3 id="2016-attention">2016: Attention</h3> <p>An intuitive neural module called Attention was developed in 2016, and showed great improvement on Machine Translation task when applied on top of the encoder hidden states of Seq2Seq models.</p> <h3 id="2017-transformers">2017: Transformers</h3> <p>In a truly ground-breaking paper ‚ÄúAttention is All You Need‚Äù, a model architecture called Transformers was introduced, which substituted all recurrent operation over the sequences of RNN with a technique called self-attention.</p> <h3 id="2018-gpt-bert---era-of-pre-trained-language-models">2018: GPT, BERT - Era of Pre-Trained Language Models</h3> <p>2018 was the era of Pre-Trained Language Models (PTLMs), which started with Generative PreTraining (GPT) models, by OpenAI. This model only had transformer decoder stacks, and was trained using traditional, unidirectional Language Modeling Objective, or Next Token Prediction.</p> <p>Google soon followed with Bidirectional Encoder Representations from Transformers (BERT), with a novel task of Masked Language Modeling (MLM). Given a text, 30% of the tokens (words) were randomly masked, and the model was trained to predict what the tokens are given other words in the text. BERT is a bidirectional encoder-only architecture.</p> <p>GPT is trained to predict next token based on previous tokens, so it can be used to generate text, auto-regressively. Therefore, it became the signature model for Natural Language Generation (NLG).</p> <p>On the other Hand, BERT‚Äôs MLM objective allowed the model to model both the forward (next token given previous tokens) and backward (previous token given future tokens) contexts, (hence, bidirectional), it could obtain higher performance in text-classification-style tasks, dominating most of Natural Language Understanding (NLU) benchmarks.</p> <p>Note that both next token prediction and MLM objective do not require any label, but vast amount of text corpus. Therefore, this method was called self-supervised learning (SSL). Both BERT and GPT were pre-trained using SSL on large corpus and obtained considerable amount of natural language knowledge encoded within them. From this point on, the major direction of research was to</p> <ol> <li>Create another PTLM <ul> <li>This entails one or many of the following: <ul> <li>creating a new dataset (cleaner &amp; larger)</li> <li>creating a new objective</li> <li>creating a new model architecture</li> </ul> </li> </ul> </li> <li>Fine-tune the PTLM for another task <ul> <li>Fine-tuning refers to the training of Pre-Trained models on a smaller, task-oriented dataset. This approach achived much better result than training a new model from scratch on the task dataset.</li> </ul> </li> </ol> <h3 id="2019-larger-models">2019: Larger Models</h3> <p>Better PTLMs, with more dataset and more parameters were released, such as GPT2, roberta-large, etc. They went to upwards of <strong>2~3B</strong> parameters.</p> <ul> <li>In today‚Äôs terms, 2~3B is not ‚Äúlarge‚Äù at all.</li> </ul> <h2 id="2020-gpt3-t5">2020: GPT3, T5</h2> <h3 id="gpt3">GPT3</h3> <p>OpenAI‚Äôs GPT3 was released. However, GPT3 was not just a model; it brought forth a lot of changes in the research directions of NLP.</p> <h4 id="closed-source">Closed Source</h4> <p>GPT3 was released as a closed-source, API only model, that users have to pay. This was the moment where OpenAI stopped being open.</p> <h4 id="pretraining-with-code-data">Pretraining with Code data</h4> <p>GPT3 pretraining data include code from github. Training on code data allowed the model to obtain some level of abstractive reasoning abilities.</p> <ul> <li>This led to the launch of Co-Pilot too.</li> </ul> <h4 id="rise-of-llm--in-context-learning">Rise of LLM &amp; In-Context Learning</h4> <p>The term Large Language Models (LLMs) started to catch fire, since GPT3 was truly large, with whopping <strong>175B</strong> parameters. With such huge scale came an unexpected ability of these models, which was called few-shot exemplar learning, which later was generalized to a term called In-Context Learning.</p> <p>famous LLMs are:</p> <ul> <li>OPT (facebook, 175B)</li> <li>PALM (GOogle, 540B)</li> <li>GPT-NeoX / GPT-J (EleutherAI)</li> <li>Bloom (Bigscience)</li> </ul> <h4 id="prompt-engineering">Prompt Engineering</h4> <p>Since GPT3 was only accessible through an API that takes textual input, what text prompt that goes into the model was important to obtain better output. This led to a subfield named prompt engineering, where various algorithms were proposed to find a textual prompt that would allow users to use GPT3 (or sometimes other models) better.</p> <ul> <li>This later inspired the rise of Prompt Tuning</li> </ul> <h3 id="t5">T5</h3> <p>All tasks represented in text and solved with single transformer encoder-decoder architecture.</p> <ul> <li>I think this laid the foundation for instruction tuning later.</li> <li>There is a similar model called T0 from Bigscience, which is pretrained to allow zero-shot inference on many NLP tasks using the same architecture as T5.</li> </ul> <h3 id="2021">2021</h3> <h4 id="parameter-efficient-fine-tuning-peft">Parameter Efficient Fine-Tuning (PEFT)</h4> <p>With the SOTA models being extra large, it was increasingly difficult to train these models on reasonable amount of GPUs. Also, tuning the entire model often resulted in problems like catastrophic forgetting. Researchers started to come up with ideas later organized as <strong>Parameter Efficient Fine-Tuning (PEFT)</strong>, which freezes the entire model weights except for a few parts (or introduce some small extra weights) and only tune that part on the task-oriented data.</p> <p>Most of the time, PEFT under-performed full finetuning. However, Prompt Tuning showed above SOTA performance in many benchmarks, and took the community by a storm.</p> <p>Prompt Tuning, or Soft Prompt Tuning, is a method of tuning a set of continuous vectors in the embedding / latent space of the model, that are prepended to the text embedding matrix. This way, the model itself is entirely frozen, and only a small set of vectors prepended to the input are trained using the gradient that flows through the model. This method achieved SOTA in many NLU tasks, taking the NLP trend of 2021.</p> <h4 id="chain-of-thought-prompting">Chain of Thought Prompting</h4> <p>This prompting guided the model to ‚Äúthink step by step‚Äù: this allowed the model to reason sequential steps to solving a problem, resulting in a more coherent reasoning of hard problems.</p> <h3 id="2022-instruction-tuning">2022: Instruction Tuning</h3> <p>Instruction tuning is sometimes called an alignment task: aligning language models with human usage of them. Most of the time, users ‚Äúasks‚Äù or ‚Äúinstructs‚Äù the language models to generate some text; therefore, training these models to follow textual instruction can greatly increase their usability.</p> <p>Similar to the idea of T5, all different NLP tasks are represented as text, prepended with a natural language instruction of how to solve the task. This way, the LMs can model the dependency between the instruction, problem, and the answer, and since all of them are text, they become stronger at generalization and obtain strong zero-shot abilities.</p> <p>Notable models are:</p> <ul> <li>OPT-IML (Facebook)</li> <li>FLAN, Flan-T5 (Google)</li> <li>InstructGPT (GPT 3.5 family), ChatGPT (OpenAI)</li> <li>T0 (Bigscience, kind of)</li> </ul> <h3 id="chatgpt">ChatGPT</h3> <p>ChatGPT deserves its own section, because it was THE AI Phenomenon of 2000s.</p> <p>On top of InstructGPT, it was trained using Reinforcement Learning from Human Feedbacks (RLHF); basically, they hired a lot of human annotators (allegedly 1000) to ‚Äútalk‚Äù with the current state of the model and give feedback in the form of score &amp; suggested answer. This way, ChatGPT could really generate truly human-level text. Also, thanks to 175B parameter scale, it stored tremendous amount of world knowledge, and can nearly be used as a search engine.</p> <p>However, it is still limited in that it ‚Äúhallucinates‚Äù: it generates very plausible text that is entirely false.</p>]]></content><author><name></name></author><category term="Review"/><category term="NLP"/><category term="LM"/><category term="AI"/><category term="ML"/><category term="Deep-learning"/><summary type="html"><![CDATA[Recent trends in NLP from Attention to ChatGPT]]></summary></entry><entry><title type="html">AINize + Gradio: Easiest Automatic ML model deployment</title><link href="syncdoth.github.io/blog/2022/ainize_gradio/" rel="alternate" type="text/html" title="AINize + Gradio: Easiest Automatic ML model deployment"/><published>2022-06-24T00:12:00+00:00</published><updated>2022-06-24T00:12:00+00:00</updated><id>syncdoth.github.io/blog/2022/ainize_gradio</id><content type="html" xml:base="syncdoth.github.io/blog/2022/ainize_gradio/"><![CDATA[<p>Machine Learning is great. You can train models to do a lot of fancy things, like analyze the text for you, or generate images from descriptions. However, in their raw forms, what they do is simply take numbers and return numbers, which are uninterpretable to naked human eyes. Most of the time, to show off your model, you would need some demo page that can interactively take input and generate output from and to users with ease. This has been very difficult before: you would need to</p> <ol> <li>Build inference server (REST or grpc)</li> <li>Deploy to Backend Server</li> <li>Build a frontend page and connect it to the inference server</li> </ol> <p>Beling AI developers/researchers, these works might have been too much of an extra work. However, I stumpled upon two awesome projects that make these procedures much easier!</p> <h2 id="what-are-they">What are they?</h2> <h3 id="gradio">Gradio</h3> <p><a href="https://gradio.app">Gradio</a> is an open source project that generates a simple frontend to demonstrate your ML model wrapped in a inference function. It also sets up a REST API that can be called from http requests. This is used in <a href="https://huggingface.co/models">huggingface hub</a>.</p> <h3 id="ainize">AINize</h3> <p><a href="https://ainize.ai/">AINize</a> is a free service that automatically deploys the connected github repository based on the repository‚Äôs dockerfile. You can use this to serve anything, really! (it doesn‚Äôt even have to be AI or ML related: just anything that can be dockerized)</p> <h2 id="step-1-setup-gradio-service">Step 1. Setup Gradio Service</h2> <p>A sample Gradio service interface has this syntax:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">gradio</span> <span class="k">as</span> <span class="n">gr</span>

<span class="n">iface</span> <span class="o">=</span> <span class="n">gr</span><span class="p">.</span><span class="nc">Interface</span><span class="p">(</span>
           <span class="n">fn</span><span class="o">=</span><span class="n">model_infer_fn</span><span class="p">,</span>
           <span class="n">inputs</span><span class="o">=</span><span class="p">[</span>
               <span class="n">gr</span><span class="p">.</span><span class="nc">Textbox</span><span class="p">(</span><span class="n">lines</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">placeholder</span><span class="o">=</span><span class="sh">"</span><span class="s">text here...</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">sentence</span><span class="sh">'</span><span class="p">),</span>
               <span class="n">gr</span><span class="p">.</span><span class="nc">Slider</span><span class="p">(</span><span class="n">minimum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                <span class="n">maximum</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                                <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                <span class="n">default</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                                <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">threshold</span><span class="sh">'</span><span class="p">),</span>
           <span class="p">],</span>
           <span class="n">outputs</span><span class="o">=</span><span class="p">[</span>
               <span class="n">gr</span><span class="p">.</span><span class="nc">Textbox</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">prediction</span><span class="sh">"</span><span class="p">),</span>
           <span class="p">],</span>
           <span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s">Demo</span><span class="sh">'</span><span class="p">,</span>
           <span class="n">theme</span><span class="o">=</span><span class="sh">'</span><span class="s">peach</span><span class="sh">'</span><span class="p">,</span>
        <span class="p">)</span>
 <span class="n">iface</span><span class="p">.</span><span class="nf">launch</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>This creates an interface with:</p> <ul> <li>2 inputs: 1 string input and 1 slider input (float values)</li> <li>1 output panel: a textbox</li> </ul> <p><em>You can take a look at different options for input and outputs in the</em> <em><a href="https://gradio.app/docs/#components">official documentation</a>.</em></p> <p>Also, you would need to define <code class="language-plaintext highlighter-rouge">model_infer_fn</code>, which takes the inputs (1 string and 1 float) and returns a string. A simple example would be:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre><span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">()</span>  <span class="c1"># some model you trained
</span>
<span class="k">def</span> <span class="nf">model_infer_fn</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="sh">'</span><span class="s">positive</span><span class="sh">'</span> <span class="k">if</span> <span class="n">pred</span> <span class="o">&gt;=</span> <span class="n">treshold</span> <span class="k">else</span> <span class="sh">'</span><span class="s">negative</span><span class="sh">'</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>For me, I like to create a <code class="language-plaintext highlighter-rouge">ModelInterface</code> class for this:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">ModelInterface</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="nc">ModelClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="c1"># process output: text, image, plot, etc.
</span>        <span class="n">output</span> <span class="o">=</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">interpret</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        returns the contribution of each input component.
        </span><span class="sh">"""</span>
        <span class="k">pass</span>

<span class="bp">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">ModelInterface</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">iface</span> <span class="o">=</span> <span class="n">gr</span><span class="p">.</span><span class="n">Interface</span><span class="p">(</span>
           <span class="n">fn</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">infer</span><span class="p">,</span>
<span class="bp">...</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Putting the <code class="language-plaintext highlighter-rouge">gr.Interface</code> in the main function and <code class="language-plaintext highlighter-rouge">ModelInterface</code> together, the <code class="language-plaintext highlighter-rouge">serve.py</code> file would look something like this:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
</pre></td><td class="code"><pre> <span class="kn">import</span> <span class="n">argparse</span>

 <span class="kn">import</span> <span class="n">gradio</span> <span class="k">as</span> <span class="n">gr</span>

 <span class="c1"># your trained model class
</span> <span class="kn">from</span> <span class="n">models</span> <span class="kn">import</span> <span class="n">ModelClass</span>


 <span class="k">class</span> <span class="nc">ModelInterface</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="nc">ModelClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="c1"># process output: text, image, plot, etc.
</span>        <span class="n">output</span> <span class="o">=</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">interpret</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        returns the contribution of each input component.
        </span><span class="sh">"""</span>
        <span class="k">pass</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="p">.</span><span class="nc">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--model_name</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse_args</span><span class="p">()</span>
    <span class="n">model</span> <span class="o">=</span> <span class="nc">ModelInterface</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">model_name</span><span class="p">)</span>

    <span class="n">iface</span> <span class="o">=</span> <span class="n">gr</span><span class="p">.</span><span class="nc">Interface</span><span class="p">(</span>
        <span class="n">fn</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">infer</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">gr</span><span class="p">.</span><span class="nc">Textbox</span><span class="p">(</span><span class="n">lines</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">placeholder</span><span class="o">=</span><span class="sh">"</span><span class="s">text here...</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">sentence</span><span class="sh">'</span><span class="p">),</span>
            <span class="n">gr</span><span class="p">.</span><span class="nc">Slider</span><span class="p">(</span><span class="n">minimum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                             <span class="n">maximum</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                             <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                             <span class="n">default</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                             <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">threshold</span><span class="sh">'</span><span class="p">),</span>
        <span class="p">],</span>
        <span class="n">outputs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">gr</span><span class="p">.</span><span class="nc">Textbox</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">prediction</span><span class="sh">"</span><span class="p">),</span>
        <span class="p">],</span>
        <span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s">Demo</span><span class="sh">'</span><span class="p">,</span>
        <span class="n">theme</span><span class="o">=</span><span class="sh">'</span><span class="s">peach</span><span class="sh">'</span><span class="p">,</span>
        <span class="n">interpretation</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">interpret</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">iface</span><span class="p">.</span><span class="nf">launch</span><span class="p">(</span><span class="n">server_name</span><span class="o">=</span><span class="sh">"</span><span class="s">0.0.0.0</span><span class="sh">"</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="nf">main</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <h2 id="step-2-dockerize">Step 2. Dockerize</h2> <p>AINize works with Dockerized repositories, and finds entry point from the repository‚Äôs <code class="language-plaintext highlighter-rouge">DOCKERFILE</code> in the root directory. We can setup the <code class="language-plaintext highlighter-rouge">DOCKERFILE</code> as follows:</p> <figure class="highlight"><pre><code class="language-docker" data-lang="docker"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre><span class="k">FROM</span><span class="s"> pytorch/pytorch:1.8.1-cuda10.2-cudnn7-devel  # base docker image from docker hub</span>
<span class="k">RUN </span>pip <span class="nb">install </span>pip <span class="nt">--upgrade</span>
<span class="k">RUN </span>pip <span class="nb">install </span>gradio Jinja2  <span class="c"># install required packages</span>

<span class="c"># setup directory</span>
<span class="k">COPY</span><span class="s"> . /app</span>
<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="c"># gradio uses port 7860 by default</span>
<span class="k">EXPOSE</span><span class="s"> 7860</span>

<span class="c"># run the serve.py</span>
<span class="k">CMD</span><span class="s"> ["python", "serve.py"]</span>
</pre></td></tr></tbody></table></code></pre></figure> <h2 id="step-3-deploy-to-ainize">Step 3. Deploy to AINize</h2> <p>Go to AINize dashboard.</p> <ol> <li>Copy and paste the url of your github repo to search bar.</li> <li>Select the branch for deployment.</li> <li>Wait until the setup is over.</li> <li>In the deployment dashboard, click the pencil button to edit environment variables and add 7860 to ports.</li> </ol> <p>Voila! You now have deployed your service to the web, with both the frontend demo and backend REST API setup!</p>]]></content><author><name></name></author><category term="Tutorial"/><category term="machine-learning"/><category term="deployment"/><category term="docker"/><summary type="html"><![CDATA[Serving a simple demo page (along with REST API) for you ML model on the web.]]></summary></entry><entry><title type="html">Mathematical Notations for Democracy</title><link href="syncdoth.github.io/blog/2022/math_democracy/" rel="alternate" type="text/html" title="Mathematical Notations for Democracy"/><published>2022-06-20T00:53:00+00:00</published><updated>2022-06-20T00:53:00+00:00</updated><id>syncdoth.github.io/blog/2022/math_democracy</id><content type="html" xml:base="syncdoth.github.io/blog/2022/math_democracy/"><![CDATA[<p>As an engineer / scientist, it is sometimes easier for me to express my thoughts with solid mathematical definition and models.</p> <h2 id="objective-function-of-a-society">Objective Function of a Society</h2> <p>What is the objective of any kind of social structure or ideology? I believe it is to provide a framework for making decisions that set the course of humanity towards a certain direction. Ideally, we want the directions to point toward something ultimately ‚Äúgood‚Äù for humanity, be it justice, utopia, or whatever you want to call it.</p> <p>Let‚Äôs express this mathematically. Suppose there exists a space \(S \subset \mathbb{R}^n\) such that can express the state of humankind, i.e., there are \(n\) different independent variables that can succinctly and sufficiently capture all aspects of the humanity. (This is obviously impossible in the real world, but let‚Äôs suppose this is possible.)</p> <p>Let \(s \in S\) be the state of humanity at some point. We believe there exists a singular ideal state for the humankind, \(u \in S\). Then, the objective function of any rational society will be:</p> \[J = \min_{s}(\|s - u\|)\] <p>which means that the goal of a society is to find state \(s\) such that it is the closest to (or reaches) the utopian state \(u\).</p> <p>In real life, there is a big problem with this objective function: we do not know \(u\). Therefore, real life social architectures or societal algorithms should make some assumptions and try to approximate \(u\).</p> <h2 id="democratic-algorithm">Democratic Algorithm</h2> <p>The way we do this in democracy is kind of similar to gradient descent. In gradient descent, we take the instantaneous gradient from the current state‚Äôs point on the loss surface, and move toward that direction. In democracy, we ask everyone what they think, and we take the direction that the most people agree with. So, we make policies and progress as a society in that direction iteratively. If we call this direction \(d \in \mathbb{R}^n\), this iterative algorithm can be expressed as the following:</p> \[s_{t+1} := s_t + d_t\] <p>So now the question is, how do we get \(d\)?</p> <h3 id="1-equal-vote">1. Equal Vote</h3> <p>In the most pure form of democracy, everyone should get an equal vote, and the direction should be just the sum of all opinions. Let \(P \subset \mathbb{R}^n\) be a set of people in a society, and let \(v_i \in P\) be an opinion vector, which represents person \(i\)‚Äôs opinion on what the humanity should do, or progress toward, to reach utopia. Then, \(d\) should simply be:</p> \[d = \sum v_i\] <h3 id="2--meritocracy">2. + Meritocracy</h3> <p>In a more realistic model, some people‚Äôs opinion could carry more weight, depending on their intelligence, education level, social status, etc. Let \(w \in \mathbb{R}^{ \mid P \mid }\) be a weight vector, which leads to \(d\) being:</p> \[d = \sum_i^{ |P| } w_i v_i\] <h3 id="3--representativeness--delegation">3. + Representativeness / Delegation</h3> <p>Being even more practical, in real world, we have representative democracy, in which the public delegates their ability to directly influence the direction \(d\) (which is incarnated in the form of a policy) to certain politicians. Therefore, \(w_i \approx 0\) for most people except for a few politicians.</p> <h3 id="4-practical-implementation">4. Practical Implementation</h3> <p>However, this is still ideal: in real world, we cannot take the vector sum of opinions like as we do in math. Usually what happens is that we vote and select one vector that the most people agreed on. If we consider the weight vector \(w\) to carry the number of votes that certain opinion got, \(d\) is defined as</p> \[d = v_i, \text{ where } i = \arg \max (w)\] <h2 id="analysis">Analysis</h2> <p>The theoretical guarantee of a democracy is that any given time \(t\), \(d_t\) is something that at least the majority of the population (or the delegated population) agreed on.</p> <p>However, similar to gradient descent having no guarantee of converging at the global maximum if the loss plane is not ‚Äúsmooth‚Äù, the democratic algorithm does not guarantee reaching \(u\). In other words, we cannot guarantee the following:</p> \[s_0 + \sum_t^T d_t = u\] <p>This is because:</p> <ol> <li>People cannot actually know where \(u\) is.</li> <li>\(d_t\) is solely decided through consensus of the population \(P\) at time \(t\).</li> </ol> <p>The democratic algorithm will converge to a state \(s_t\) if \(d_t = 0,\) which happens when the population \(P_t\) agrees that \(s_t\) is ‚Äúas good as it can get‚Äù, or agrees that \(s_t = u\), even if this is false!</p> <p>This means that the best we could do with democracy is to find some ‚Äúlocal minimum‚Äù, where the majority of the population do not want change, and agrees that it is as good as it can get for the mankind.</p> <p>In other words, the assumption of democratic algorithm is that the ultimate consensus state is equivalent to \(u\).</p>]]></content><author><name></name></author><category term="Ideas"/><category term="politics"/><category term="democracy"/><summary type="html"><![CDATA[This is my attempt to express the core idea of democracy in terms of math.]]></summary></entry><entry><title type="html">Introducing easy HW acceleration for PyTorch</title><link href="syncdoth.github.io/blog/2021/hf_accelerate/" rel="alternate" type="text/html" title="Introducing easy HW acceleration for PyTorch"/><published>2021-07-30T00:16:00+00:00</published><updated>2021-07-30T00:16:00+00:00</updated><id>syncdoth.github.io/blog/2021/hf_accelerate</id><content type="html" xml:base="syncdoth.github.io/blog/2021/hf_accelerate/"><![CDATA[<h2 id="multi-gpu-with-pytorch">Multi-GPU with PyTorch</h2> <p>The two most popular deep learning frameworks are arguably Tensorflow and PyTorch. There are many difference and similarities between the two, and one notable difference is about HW acceleration device management. For Tensorflow, after they updated their API to V2.0, most of the device management is done automatically: in most cases, the default behaviour is to use all the available gpus possible, and no explicit device placement code for each tensor object is needed. However, in PyTorch, the default is to not use any HW acceleration, and the device placement for each tensor is manual. This makes the tensor‚Äôs placement explicit and easy to follow, yet difficult and tedious in many occasions. For example, multi-GPU setting or TPU setting with PyTorch requires quite a bit of knowledge about multi-process (or multi-threaded) programming, distributed computing, etc. all of which are done in the background of the tensorflow engine.</p> <h2 id="introducing-accelerate">Introducing accelerate</h2> <p>Recently, Huggingface released a library called <a href="https://github.com/huggingface/accelerate">accelerate</a>, which, as the name suggests, contains ready-to-use APIs for hardware accelerations. What‚Äôs notable about this library is that it only requires few lines of code changed from vanilla PyTorch code! An example from the official <a href="https://github.com/huggingface/accelerate/blob/main/README.md">repo</a>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td><td class="code"><pre>  <span class="kn">import</span> <span class="n">torch</span>
  <span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
  <span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="o">+</span> <span class="kn">from</span> <span class="n">accelerate</span> <span class="kn">import</span> <span class="n">Accelerator</span>

<span class="o">-</span> <span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>
<span class="o">+</span> <span class="n">accelerator</span> <span class="o">=</span> <span class="nc">Accelerator</span><span class="p">()</span>

<span class="o">-</span> <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Transformer</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="o">+</span> <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Transformer</span><span class="p">()</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>

  <span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">'</span><span class="s">my_dataset</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="o">+</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">accelerator</span><span class="p">.</span><span class="nf">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

  <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">source</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
<span class="o">-</span>         <span class="n">source</span> <span class="o">=</span> <span class="n">source</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="o">-</span>         <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

          <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

          <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

<span class="o">-</span>         <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="o">+</span>         <span class="n">accelerator</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

          <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <h3 id="structure">Structure</h3> <hr/> <p><em>Currently, the documentation of <code class="language-plaintext highlighter-rouge">accelerate</code> is not complete: except for the main</em> <em>class and examples, there is almost no documentation for different classes.</em></p> <hr/> <p>The main class is <code class="language-plaintext highlighter-rouge">Accelerator</code>, defined <a href="https://github.com/huggingface/accelerate/blob/main/src/accelerate/accelerator.py">here</a>. The main methods include:</p> <ul> <li><code class="language-plaintext highlighter-rouge">prepare</code>: it prepares the model, dataloader, and optimizer for the specified device configuration. After this, the model is wrapped as <code class="language-plaintext highlighter-rouge">torch.DistributedDataParallel</code> object.</li> <li><code class="language-plaintext highlighter-rouge">backward</code>: instead of <code class="language-plaintext highlighter-rouge">loss.backward()</code>, <code class="language-plaintext highlighter-rouge">accelerator.backward(loss)</code> should be called.</li> <li><code class="language-plaintext highlighter-rouge">print</code>: instead of plain python <code class="language-plaintext highlighter-rouge">print()</code>, <code class="language-plaintext highlighter-rouge">accelerator.print()</code> should be called, so that it is printed only once in the main process.</li> <li><code class="language-plaintext highlighter-rouge">gather</code>: gathers tensors in different devices into one place.</li> <li><code class="language-plaintext highlighter-rouge">save</code>: instead of <code class="language-plaintext highlighter-rouge">torch.save(model, path)</code>, <code class="language-plaintext highlighter-rouge">accelerator.save(model, path)</code> should be called. This saves the model only once in the main process.</li> </ul> <p><code class="language-plaintext highlighter-rouge">gather</code> method is mostly used with <strong>distributed evaluation</strong>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre>  <span class="n">preds</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">targets</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">validation_dataloader</span><span class="p">:</span>
      <span class="n">predictions</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
      <span class="c1"># Gather all predictions and targets
</span>      <span class="n">preds</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">accelerator</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">predictions</span><span class="p">))</span>
      <span class="n">targets</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">accelerator</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">targets</span><span class="p">))</span>

  <span class="c1"># perform evaluation...</span>
</pre></td></tr></tbody></table></code></pre></figure> <h3 id="tips">Tips</h3> <p>Always remember that the <code class="language-plaintext highlighter-rouge">model</code> that you <code class="language-plaintext highlighter-rouge">prepare</code>d is now a <code class="language-plaintext highlighter-rouge">torch.DistributedDataParallel</code> object; you will not be able to call class specific methods of the <code class="language-plaintext highlighter-rouge">model</code>, such as <code class="language-plaintext highlighter-rouge">save_pretrained</code> of the huggingface <code class="language-plaintext highlighter-rouge">transformers.PretrainedModel</code> class. Also, note that the model saved with <code class="language-plaintext highlighter-rouge">accelerator.save</code> is also a <code class="language-plaintext highlighter-rouge">torch.DistributedDataParallel</code> object, and requires distributed setting when loading, which might be a little bit of a pain in the neck. You can refer to this <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-checkpoints">tutorial</a> for what setup is needed to load the model.</p> <p>It is difficult to avoid this problem without looking at the source code: referring to the <code class="language-plaintext highlighter-rouge">accelerator.save</code> at <a href="https://github.com/huggingface/accelerate/blob/b08fd560a4d6b7427f9fbb51a767393699afbd95/src/accelerate/utils.py#L270">here</a>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre>  <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
      <span class="sh">"""</span><span class="s">
      Save the data to disk. Use in place of :obj:`torch.save()`.
      Args:
          obj: The data to save
          f: The file (or file-like object) to use to save the data
      </span><span class="sh">"""</span>
      <span class="k">if</span> <span class="nc">AcceleratorState</span><span class="p">().</span><span class="n">distributed_type</span> <span class="o">==</span> <span class="n">DistributedType</span><span class="p">.</span><span class="n">TPU</span><span class="p">:</span>
          <span class="n">xm</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
      <span class="k">elif</span> <span class="nc">AcceleratorState</span><span class="p">().</span><span class="n">local_process_index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
          <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Note that <code class="language-plaintext highlighter-rouge">AcceleratorState()</code> is not a class bound property, but a singleton class that all instances of this class have the same state. Therefore, we can write a custom code to save the actual model wrapped by <code class="language-plaintext highlighter-rouge">torch.DistributedDataParallel</code>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="code"><pre>  <span class="kn">from</span> <span class="n">accelerate.state</span> <span class="kn">import</span> <span class="n">AcceleratorState</span>
  <span class="kn">from</span> <span class="n">accelerate.utils</span> <span class="kn">import</span> <span class="n">save</span>

  <span class="k">def</span> <span class="nf">custom_save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">save_module_only</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
      <span class="sh">"""</span><span class="s">
      Save the data to disk. Use in place of :obj:`torch.save()`.
      Args:
          model: pytorch model. DistributedDataParallel class.
          f: The file (or file-like object) to use to save the data
          save_module_only: save the wrapped module only.
      </span><span class="sh">"""</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">save_module_only</span><span class="p">:</span>
          <span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
          <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">module</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

  <span class="c1"># save
</span>  <span class="nf">custom_save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="sh">'</span><span class="s">model.pt</span><span class="sh">'</span><span class="p">,</span> <span class="n">save_module_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="c1"># ...
</span>  <span class="c1"># load
</span>  <span class="n">loaded_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">model.pt</span><span class="sh">'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <h2 id="alternatives">Alternatives</h2> <ul> <li> <p>Huggingface <code class="language-plaintext highlighter-rouge">transformers</code> already provides <code class="language-plaintext highlighter-rouge">Trainer</code> module, which takes care of hardware accelerations automatically. However, this is pretty much made only for the models provided in the <code class="language-plaintext highlighter-rouge">transformers</code> library, so it is less general.</p> </li> <li> <p>Another great alternative is <a href="https://www.pytorchlightning.ai/">pytorch-lightning</a> module. This is analogous to <code class="language-plaintext highlighter-rouge">keras</code> library made for PyTorch: the actual gradient update procedures are hidden from the developers, allowing them to focus only on the model‚Äôs architecture. This is indeed a great module and provides much more than just hardware acceleration; logging, tensorboard, checkpointing, training loop, and many other non-modeling codes are provided with simple options to the API. The only downside is that one has to refactor the code quite significantly to the required format of the module, which might not be feasible in the later stages of the project. For example, when I am working on a NLP related task, my go-to model would be <code class="language-plaintext highlighter-rouge">bert-base-cased</code>. This model can fit quite well within one GPU with 12GB of vRAM, so I wouldn‚Äôt worry about multi-gpu when I start coding up the project. However, as the project proceeds, I might require larger models, longer text, larger batch size to save time, and whatever reason that calls for multi-gpu setting. In that case, completely revamping the code to fit with <code class="language-plaintext highlighter-rouge">pytorch-lightning</code> module might be too much of a work, while using <code class="language-plaintext highlighter-rouge">accelerate</code> requries only a few lines of code modified.</p> </li> </ul>]]></content><author><name></name></author><category term="Tutorial"/><category term="machine-learning"/><summary type="html"><![CDATA[Multi-GPU / TPU setting with PyTorch made easy with huggingface accelerate.]]></summary></entry><entry><title type="html">How to serve Pororo Models</title><link href="syncdoth.github.io/blog/2021/mrc_serve/" rel="alternate" type="text/html" title="How to serve Pororo Models"/><published>2021-05-04T00:19:00+00:00</published><updated>2021-05-04T00:19:00+00:00</updated><id>syncdoth.github.io/blog/2021/mrc_serve</id><content type="html" xml:base="syncdoth.github.io/blog/2021/mrc_serve/"><![CDATA[<p>Recently, Kakao Brain Corp. has released a very interesting open source library called <code class="language-plaintext highlighter-rouge">Pororo</code>, named after a very famous Korean animation character. This library provides various number of natural language related models and APIs to easily apply them to your services, especially for Korean applications.</p> <p>One complication is that the level of abstraction is a double edged sword: it allows for easy usage, but it hinders manual tweaking. My purpose here is to serve their model: which means that I would have to track down towards their actual model save file and prediction logics, etc. Let get to it.</p> <h2 id="specifications-and-dependencies">Specifications and Dependencies</h2> <p>The library is mainly built on 2 packages: <code class="language-plaintext highlighter-rouge">fairseq</code> and <code class="language-plaintext highlighter-rouge">transformers</code>, which both is based on <code class="language-plaintext highlighter-rouge">PyTorch</code> framework. Therefore, <code class="language-plaintext highlighter-rouge">torchserve</code> will be used to serve these models.</p> <p>Also, it requires <code class="language-plaintext highlighter-rouge">openjdk</code>, which is a required package from <code class="language-plaintext highlighter-rouge">mecab</code>, a popular morphology analysis tool originally for Japanese, but also ported to Korean.</p> <h2 id="implementation">Implementation</h2> <p>The code is based on a tutorial given in the following <a href="https://towardsdatascience.com/serving-pytorch-models-with-torchserve-6b8e8cbdb632">blog</a>. Few adjustments have been made in the model definition part in the <code class="language-plaintext highlighter-rouge">handler.py</code>, specific to the <code class="language-plaintext highlighter-rouge">Pororo</code> package.</p> <p>According to the official <code class="language-plaintext highlighter-rouge">Pororo</code> documentation, the simplest way to start a MRC task is by:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="o">&gt;&gt;&gt;</span> <span class="n">mrc</span> <span class="o">=</span> <span class="nc">Pororo</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">mrc</span><span class="sh">"</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="sh">"</span><span class="s">ko</span><span class="sh">"</span><span class="p">)</span>
    <span class="o">&gt;&gt;&gt;</span> <span class="nf">mrc</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">Ïπ¥Ïπ¥Ïò§Î∏åÎ†àÏù∏Ïù¥ Í≥µÍ∞úÌïú Í≤ÉÏùÄ?</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Ïπ¥Ïπ¥Ïò§ Ïù∏Í≥µÏßÄÎä•(AI) Ïó∞Íµ¨Í∞úÎ∞ú ÏûêÌöåÏÇ¨ Ïπ¥Ïπ¥Ïò§Î∏åÎ†àÏù∏Ïù¥ AI ÏÜîÎ£®ÏÖòÏùÑ Ï≤´ ÏÉÅÌíàÌôîÌñàÎã§. Ïπ¥Ïπ¥Ïò§Îäî Ïπ¥Ïπ¥Ïò§Î∏åÎ†àÏù∏ </span><span class="sh">'</span><span class="s">Ìè¨Ï¶à(pose¬∑ÏûêÏÑ∏Î∂ÑÏÑù) API</span><span class="sh">'</span><span class="s">Î•º Ïú†Î£å Í≥µÍ∞úÌïúÎã§Í≥† 24Ïùº Î∞ùÌòîÎã§. Ïπ¥Ïπ¥Ïò§Î∏åÎ†àÏù∏Ïù¥ AI Í∏∞Ïà†ÏùÑ Ïú†Î£å APIÎ•º Í≥µÍ∞úÌïòÎäî Í≤ÉÏùÄ Ï≤òÏùåÏù¥Îã§. Í≥µÍ∞úÌïòÏûêÎßàÏûê Ïô∏Î∂Ä Î¨∏ÏùòÍ∞Ä ÏáÑÎèÑÌïúÎã§. Ìè¨Ï¶àÎäî AI ÎπÑÏ†Ñ(VISION, ÏòÅÏÉÅ¬∑ÌôîÎ©¥Î∂ÑÏÑù) Î∂ÑÏïº Ï§ë ÌïòÎÇòÎã§. Ïπ¥Ïπ¥Ïò§Î∏åÎ†àÏù∏ Ìè¨Ï¶à APIÎäî Ïù¥ÎØ∏ÏßÄÎÇò ÏòÅÏÉÅÏùÑ Î∂ÑÏÑùÌï¥ ÏÇ¨Îûå ÏûêÏÑ∏Î•º Ï∂îÏ∂úÌïòÎäî Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌïúÎã§.</span><span class="sh">"</span>
    <span class="p">)</span>
    <span class="o">&gt;&gt;&gt;</span> <span class="p">(</span><span class="sh">'</span><span class="s">Ìè¨Ï¶à(pose¬∑ÏûêÏÑ∏Î∂ÑÏÑù) API</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">33</span><span class="p">,</span> <span class="mi">44</span><span class="p">))</span>
    <span class="o">&gt;&gt;&gt;</span> <span class="c1"># when mecab doesn't work well for postprocess, you can set `postprocess` option as `False`
</span>    <span class="o">&gt;&gt;&gt;</span> <span class="nf">mrc</span><span class="p">(</span><span class="sh">"</span><span class="s">Ïπ¥Ïπ¥Ïò§Î∏åÎ†àÏù∏Ïù¥ Í≥µÍ∞úÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ Ïù¥Î¶ÑÏùÄ?</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Ïπ¥Ïπ¥Ïò§Î∏åÎ†àÏù∏ÏùÄ ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨ÏôÄ ÏùåÏÑ± Í¥ÄÎ†® ÌÉúÏä§ÌÅ¨Î•º ÏâΩÍ≤å ÏàòÌñâÌï† Ïàò ÏûàÎèÑÎ°ù ÎèÑÏôÄ Ï£ºÎäî ÎùºÏù¥Î∏åÎü¨Î¶¨ pororoÎ•º Í≥µÍ∞úÌïòÏòÄÏäµÎãàÎã§.</span><span class="sh">"</span><span class="p">,</span> <span class="n">postprocess</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="p">(</span><span class="sh">'</span><span class="s">pororo</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">34</span><span class="p">))</span>
</code></pre></div></div> <p>This is similar to the <code class="language-plaintext highlighter-rouge">pipeline</code> abstraction classes provided by <code class="language-plaintext highlighter-rouge">huggingface</code>‚Äôs <code class="language-plaintext highlighter-rouge">transformers</code> library. However, our goal is to serve the actual model, and one of the requirements is to load the model from a local checkpoint file. Therefore, we need to get to the class that can actually be instantiated from a model weight file.</p> <p>First, calling <code class="language-plaintext highlighter-rouge">Pororo(task="mrc", lang="ko")</code> creates a <code class="language-plaintext highlighter-rouge">PororoMrcFactory</code> class. Then, it loads corresponding model and returns through:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre></td><td class="code"><pre>    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Load user-selected task-specific model

        Args:
            device (str): device information

        Returns:
            object: User-selected task-specific model

        </span><span class="sh">"""</span>
        <span class="k">if</span> <span class="sh">"</span><span class="s">brainbert</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">n_model</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="kn">import</span> <span class="n">mecab</span>
            <span class="k">except</span> <span class="nb">ModuleNotFoundError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">error</span><span class="p">.</span><span class="nf">__class__</span><span class="p">(</span>
                    <span class="sh">"</span><span class="s">Please install python-mecab-ko with: `pip install python-mecab-ko`</span><span class="sh">"</span>
                <span class="p">)</span>
            <span class="kn">from</span> <span class="n">pororo.models.brainbert</span> <span class="kn">import</span> <span class="n">BrainRobertaModel</span>
            <span class="kn">from</span> <span class="n">pororo.utils</span> <span class="kn">import</span> <span class="n">postprocess_span</span>

            <span class="n">model</span> <span class="o">=</span> <span class="p">(</span><span class="n">BrainRobertaModel</span><span class="p">.</span><span class="nf">load_model</span><span class="p">(</span>
                <span class="sa">f</span><span class="sh">"</span><span class="s">bert/</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">n_model</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">lang</span><span class="p">,</span>
            <span class="p">).</span><span class="nf">eval</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

            <span class="n">tagger</span> <span class="o">=</span> <span class="n">mecab</span><span class="p">.</span><span class="nc">MeCab</span><span class="p">()</span>

            <span class="k">return</span> <span class="nc">PororoBertMrc</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tagger</span><span class="p">,</span> <span class="n">postprocess_span</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Okay‚Ä¶ So we can observe two things here.</p> <ol> <li><code class="language-plaintext highlighter-rouge">BrainRobertaModel</code> should contain the actual implementation of the model.</li> <li><code class="language-plaintext highlighter-rouge">PororoBertMrc</code> class would contain some implementaions that are specific to the MRC task. Therefore, this class should be used for inference during serve!</li> </ol> <p>Now, let‚Äôs go deeper into the <code class="language-plaintext highlighter-rouge">BrainRobertaModel</code> class.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">BrainRobertaModel</span><span class="p">(</span><span class="n">RobertaModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Helper class to load pre-trained models easily. And when you call load_hub_model,
    you can use brainbert models as same as RobertaHubInterface of fairseq.
    Methods
    -------
    load_model(log_name: str): Load RobertaModel
    </span><span class="sh">"""</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">lang</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Load pre-trained model as RobertaHubInterface.
        :param model_name: model name from available_models
        :return: pre-trained model
        </span><span class="sh">"""</span>
        <span class="kn">from</span> <span class="n">fairseq</span> <span class="kn">import</span> <span class="n">hub_utils</span>

        <span class="n">ckpt_dir</span> <span class="o">=</span> <span class="nf">download_or_load</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">lang</span><span class="p">)</span>
        <span class="n">tok_path</span> <span class="o">=</span> <span class="nf">download_or_load</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">tokenizers/bpe32k.</span><span class="si">{</span><span class="n">lang</span><span class="si">}</span><span class="s">.zip</span><span class="sh">"</span><span class="p">,</span> <span class="n">lang</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">hub_utils</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
            <span class="n">ckpt_dir</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">model.pt</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">ckpt_dir</span><span class="p">,</span>
            <span class="n">load_checkpoint_heads</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="nc">BrainRobertaHubInterface</span><span class="p">(</span>
            <span class="n">x</span><span class="p">[</span><span class="sh">"</span><span class="s">args</span><span class="sh">"</span><span class="p">],</span>
            <span class="n">x</span><span class="p">[</span><span class="sh">"</span><span class="s">task</span><span class="sh">"</span><span class="p">],</span>
            <span class="n">x</span><span class="p">[</span><span class="sh">"</span><span class="s">models</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">tok_path</span><span class="p">,</span>
        <span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Again, we observe that <code class="language-plaintext highlighter-rouge">BrainRobertaModel.load_model</code> is returning another object of <code class="language-plaintext highlighter-rouge">BrainRobertaHubInterface</code>. This class inherits from <code class="language-plaintext highlighter-rouge">fairseq.models.roberta.RobertaHubInterface</code>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">RobertaHubInterface</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">A simple PyTorch Hub interface to RoBERTa.
    Usage: https://github.com/pytorch/fairseq/tree/master/examples/roberta
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">,</span> <span class="n">task</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="n">self</span><span class="p">.</span><span class="n">task</span> <span class="o">=</span> <span class="n">task</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>

        <span class="n">self</span><span class="p">.</span><span class="n">bpe</span> <span class="o">=</span> <span class="n">encoders</span><span class="p">.</span><span class="nf">build_bpe</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">bpe</span><span class="p">)</span>

        <span class="c1"># this is useful for determining the device
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">"</span><span class="s">_float_tensor</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Finally, a class that is a plain PyTorch <code class="language-plaintext highlighter-rouge">nn.Module</code>! Also, this takes the <code class="language-plaintext highlighter-rouge">model</code> parameter, which is in fact, the pretrained base language model.</p> <hr/> <p>Phew, it was a long way to go through the different layers of abstractions, but we reached the final level. So what do we need now?</p> <ol> <li>Get the pretrained weight of the base model: <code class="language-plaintext highlighter-rouge">roberta</code> in this case. We have to save the weight file (<code class="language-plaintext highlighter-rouge">*.pt</code>) and vocabulary files for the tokenizer in advance for the <code class="language-plaintext highlighter-rouge">torchserve</code> to work.</li> <li>Load the pretrained weight, and instantiate a <code class="language-plaintext highlighter-rouge">BrainRobertaHubInterface</code> object.</li> <li>using the <code class="language-plaintext highlighter-rouge">BrainRobertaHubInterface</code> object, create a <code class="language-plaintext highlighter-rouge">PororoBertMrc</code> object, which can be used as a final model class to perform inference during serve.</li> </ol> <p>The resultant <code class="language-plaintext highlighter-rouge">Handler</code> for the <code class="language-plaintext highlighter-rouge">PororoBertMrc</code> model can be found <a href="https://github.com/syncdoth/mrc_serve/blob/f95feea9c1a1715e7f84506de09e50ee82c1df36/handler.py#L23">here</a>.</p> <hr/> <p><em>One little implementation detail:</em> while loading the pretrained weight, <code class="language-plaintext highlighter-rouge">fairseq.hub_utils.from_pretrained</code> is used. This method actually hard-codes some of the path variables, and makes it hard to manipulate. One work around is to simply provide a absolute path to the file. Therefore, in <code class="language-plaintext highlighter-rouge">handler.py</code>, a global variable <a href="https://github.com/syncdoth/mrc_serve/blob/f95feea9c1a1715e7f84506de09e50ee82c1df36/handler.py#L21">DATA_PATH</a> is defined.</p> <h2 id="usage">Usage</h2> <p><em>The setup details are provided in the <code class="language-plaintext highlighter-rouge">README</code> of the following</em> <em><a href="https://www.github.com/syncdoth/mrc_serve">repository</a>.</em></p> <p>After properly set up, one can send requests to the server and receive a json object. A simple example in Python is:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">requests</span>

<span class="n">question</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Ï°∞ÏÇ¨Îêú ÏÇ¨ÎûåÏùÄ Î™áÎ™ÖÏù¥Ïïº?</span><span class="sh">"</span>
<span class="n">context</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Ï≤≠ÏôÄÎåÄÎäî ÎåÄÌÜµÎ†πÎπÑÏÑúÏã§¬∑Íµ≠Í∞ÄÏïàÎ≥¥Ïã§ ÌñâÏ†ïÍ¥ÄÍ∏â Ïù¥Ìïò ÏßÅÏõê Î∞è ÏßÅÍ≥Ñ Í∞ÄÏ°± 3,714Î™ÖÍ≥º Í≤ΩÌò∏Ï≤ò ÏßÅÏõê
            Î∞è ÏßÅÍ≥ÑÍ∞ÄÏ°± 3,458Î™ÖÏóê ÎåÄÌïú ÌÜ†ÏßÄ Í±∞Îûò ÎÇ¥Ïö©ÏùÑ ÏûêÏ≤¥ Ï°∞ÏÇ¨Ìïú Í≤∞Í≥º, Í≤ΩÌò∏Ï≤ò ÏÜåÏÜç AÏî®Í∞Ä 2017ÎÖÑ
            9Ïõî Í¥ëÎ™Ö ÏßÄÏó≠ ÌÜ†ÏßÄ 413m¬≤Î•º ÌòïÏàòÏôÄ Îß§ÏûÖÌïú ÏÇ¨Ïã§ÏùÑ ÌôïÏù∏ÌñàÎã§Í≥† Ï†ïÎßåÌò∏ Ï≤≠ÏôÄÎåÄ Íµ≠ÎØºÏÜåÌÜµÏàòÏÑùÏù¥
            Ï†ÑÌñàÎã§. AÏî®Îäî Í≤ΩÌò∏Ï≤ò Í≥ºÏû•(4Í∏â)Ïù¥Îã§. Ï≤≠ÏôÄÎåÄÎäî ÏÇ¨Ïã§ÏùÑ ÌôïÏù∏Ìïú ÏßÄÎÇú 16Ïùº Í≥ßÎ∞îÎ°ú ÎåÄÍ∏∞Î∞úÎ†π Ï°∞ÏπòÌñàÎã§.
            AÏî®Îäî Ï≤≠ÏôÄÎåÄÏóê </span><span class="sh">'</span><span class="s">Î∂ÄÎ™®Îãò Î∂ÄÏñë Î™©Ï†ÅÏúºÎ°ú Í∞ÄÏ°±Í≥º Í≥µÎèô Î™ÖÏùòÎ°ú Îß§ÏûÖÌïú ÎïÖ</span><span class="sh">'</span><span class="s">Ïù¥ÎùºÎäî Ï∑®ÏßÄÎ°ú Ìï¥Î™ÖÌñàÎã§.
            Í∑∏Îü¨ÎÇò Ï≤≠ÏôÄÎåÄ Í≥†ÏúÑ Í¥ÄÍ≥ÑÏûêÎäî </span><span class="sh">"</span><span class="s">(Ìà¨Í∏∞) ÏùòÏã¨ ÏÇ¨Î°Ä</span><span class="sh">"</span><span class="s">ÎùºÎ©∞ </span><span class="sh">"</span><span class="s">(ÏÜåÎ™Ö ÎÇ¥Ïö©ÏùÄ) Ï†úÏô∏Ìïú Ï±Ñ Í±∞Îûò ÏÇ¨Ïã§
            Î∞è Íµ¨ÏûÖÍ≥º Í¥ÄÎ†®Ìïú ÏûêÎ£åÎßå ÌäπÏàòÎ≥∏Ïóê ÎÑòÍ∏∞Í∏∞Î°ú ÌñàÎã§</span><span class="sh">"</span><span class="s">Í≥† Î∞ùÌòîÎã§. Ïù¥Î≤à ÏÇ¨Î°ÄÎäî Ï°∞ÏÇ¨Îã® Ï°∞ÏÇ¨Ïùò </span><span class="sh">'</span><span class="s">ÎßπÏ†ê</span><span class="sh">'</span><span class="s">ÏùÑ
            Í≥†Ïä§ÎûÄÌûà Î≥¥Ïó¨Ï§ÄÎã§. Ï°∞ÏÇ¨Îã®ÏùÄ Ïù¥ÎÇ† Î∞úÌëúÎ•º Ìè¨Ìï®Ìï¥ LH, Íµ≠ÌÜ†ÍµêÌÜµÎ∂Ä ÏßÅÏõê Îì± 2Îßå3,000Ïó¨ Î™ÖÏùò
            ÌÜ†ÏßÄ Í±∞Îûò ÌòÑÌô©ÏùÑ Ï†ÑÏàòÏ°∞ÏÇ¨ÌñàÏßÄÎßå, </span><span class="sh">'</span><span class="s">Î≥∏Ïù∏</span><span class="sh">'</span><span class="s">ÏúºÎ°úÎßå ÎåÄÏÉÅÏùÑ ÌïúÏ†ïÌñàÎã§. Ïù¥ ÎïåÎ¨∏Ïóê Ïù¥Î≤àÏ≤òÎüº Î∞∞Ïö∞Ïûê
            Îì± Í∞ÄÏ°± Î™ÖÏùò Í±∞Îûò ÎÇ¥Ïö©ÏùÄ ÌååÏïÖÌï† Ïàò ÏóÜÎã§. AÏî® ÌòïÏùÄ 3Í∏â ÏÉÅÎãπ ÏßÅÏõêÏúºÎ°ú LH Ï∏°Ïóê </span><span class="sh">'</span><span class="s">Í∞ÄÏ°±Ïù¥
            3Í∏∞ Ïã†ÎèÑÏãú ÎÇ¥ÏóêÏÑú ÌÜ†ÏßÄ Í±∞ÎûòÎ•º ÌñàÎã§</span><span class="sh">'</span><span class="s">Í≥† ÏûêÏßÑ Ïã†Í≥†Ìïú Í≤ÉÏúºÎ°ú Ï†ÑÌï¥Ï°åÎã§. ÎπÑÏÑúÏã§¬∑ÏïàÎ≥¥Ïã§ ÏßÅÏõê ÎåÄÏÉÅ
            Ï°∞ÏÇ¨ÏóêÏÑúÎäî 3Í∏∞ Ïã†ÎèÑÏãúÏôÄ Ïù∏Í∑º ÏßÄÏó≠ÏóêÏÑúÏùò Î∂ÄÎèôÏÇ∞ Í±∞Îûò 3Í±¥ÏùÑ ÌôïÏù∏ÌñàÏúºÎÇò, Ìà¨Í∏∞Î°úÎäî ÏùòÏã¨ÎêòÏßÄ
            ÏïäÎäîÎã§Îäî Í≤å Ï≤≠ÏôÄÎåÄ Ï∏° ÏÑ§Î™ÖÏù¥Îã§.</span><span class="sh">"""</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">question</span> <span class="o">+</span> <span class="sh">"</span><span class="s">|</span><span class="sh">"</span> <span class="o">+</span> <span class="n">context</span>

<span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="sh">"</span><span class="s">http://localhost:8888/predictions/mrc</span><span class="sh">"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">utf-8</span><span class="sh">"</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">answer:</span><span class="sh">"</span><span class="p">,</span> <span class="n">r</span><span class="p">.</span><span class="nf">json</span><span class="p">())</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">answer</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">3,458Î™Ö</span><span class="sh">'</span><span class="p">,</span> <span class="p">[</span><span class="mi">37</span><span class="p">,</span> <span class="mi">41</span><span class="p">]]</span>
</pre></td></tr></tbody></table></code></pre></figure> <h2 id="2021-update">2021 Update!</h2> <p>Recently, another cool project named <a href="https://www.gradio.app/">gradio</a> has been published. This is a Python based library that builds a simple web based UI for any function with a very easy to use APIs. Since this API works at a high level and does not require actual model binary for serving, it is much easier to use.</p>]]></content><author><name></name></author><category term="Tutorial"/><category term="machine-learning"/><category term="deployment"/><summary type="html"><![CDATA[How to serve pretrained language models provided from Pororo library by Kakao Brain Corp. with torchserve.]]></summary></entry></feed>